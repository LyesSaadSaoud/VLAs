# Vision–Language–Action (VLA) Models in Robotics

This repository collects key Vision–Language–Action (VLA) models, with details on their training data, robot platforms, summaries, and links to papers and project websites.

---

## Table of Contents



---
[2022][CLIPort:][Cliport: What and where pathways for robotic manipulation](https://proceedings.mlr.press/v164/shridhar22a/shridhar22a.pdf)  
[2022][RT-1:][Rt-1: Robotics transformer for real‑world control at scale](https://arxiv.org/abs/2212.06817)  
[2022][Gato:][A Generalist Agent](https://arxiv.org/abs/2205.06175)  
[2022][VIMA:][VIMA: General Robot Manipulation with Multimodal Prompts](https://arxiv.org/abs/2210.03094)  
[YYYY][PerAct:][Title TBD](Link TBD)  
[2022][SayCan:][Do As I Can, Not As I Say: Grounding Language in Robotic Affordances](https://arxiv.org/abs/2204.01691)  
[2023][RoboAgent:][RoboAgent: Generalist Robot Agent with Semantic and Temporal Understanding](https://arxiv.org/abs/2310.08560)  
[2023][RT-Trajectory:][Robotic Task Generalization via Hindsight Trajectory Sketches](https://arxiv.org/abs/2311.01977)  
[2023][ACT:][Learning fine‑grained bimanual manipulation with low‑cost hardware](https://arxiv.org/abs/2304.13705)  
[2023][RT-2:][Rt-2: Vision‑language‑action models transfer web knowledge to robotic control](Link TBD)  
[2023][VoxPoser:][Voxposer: Composable 3D value maps for robotic manipulation with language models](https://arxiv.org/abs/2307.05973)  
[2024][CLIP-RT:][CLIP‑RT: Learning Language‑Conditioned Robotic Policies with Natural Language Supervision](https://arxiv.org/abs/2411.00508)  
[2023][Diffusion Policy:][Diffusion Policy: Visuomotor policy learning via action diffusion](Link TBD)  
[2024][Octo:][Octo: An open‑source generalist robot policy](https://arxiv.org/abs/2405.12213)  
[2024][VLATest:][Towards testing and evaluating vision‑language manipulation: An empirical study](https://arxiv.org/abs/2409.12894)  
[2024][NaVILA:][NaVILA: Legged robot vision‑language‑action model for navigation](https://arxiv.org/abs/2412.04453)  
[2024][RoboNurse‑VLA:][RoboNurse‑VLA: Real‑time voice‑to‑action pipeline for surgical instrument handover](Link TBD)  
[2024][Mobility VLA:][Mobility VLA: Multimodal instruction navigation with topological mapping](Link TBD)  
[2024][RevLA:][RevLA: Domain adaptation adapters for robotic foundation models](Link TBD)  
[2024][Uni-NaVid:][Uni-NaVid: Video‑based VLA unifying embodied navigation tasks](Link TBD)  
[2024][RDT-1B:][RDT‑1B: 1.2B‑parameter diffusion foundation model for manipulation](Link TBD)  
[2024][RoboMamba:][RoboMamba: Mamba‑based unified VLA with linear‑time inference](Link TBD)  
[2024][Chain‑of‑Affordance:][Chain‑of‑Affordance: Sequential affordance reasoning for spatial planning](Link TBD)  
[2024][Edge VLA:][Edge VLA: Lightweight, edge‑optimized VLA for low‑power real‑time inference](Link TBD)  
[2024][OpenVLA:][OpenVLA: LORA‑fine‑tuned open‑source VLA with high-success transfer](Link TBD)  
[2024][CogACT:][CogACT: Componentized diffusion action transformer for VLA](Link TBD)  
[2024][ShowUI‑2B:][ShowUI‑2B: GUI/web navigation via screenshot grounding and token selection](Link TBD)  
[2024][Pi‑0:][Pi‑0: General robot control flow model for open‑world tasks](Link TBD)  
[2024][HiRT:][HiRT: Hierarchical planning/control separation for VLA](Link TBD)  
[2024][A3VLM:][A3VLM: Articulation‑aware affordance grounding from RGB video](Link TBD)  
[2024][SVLR:][SVLR: Modular “segment‑to‑action” pipeline using visual prompt retrieval](Link TBD)  
[2024][Bi‑VLA:][Bi‑VLA: Dual‑arm instruction‑to‑action planner for recipe demonstrations](Link TBD)  
[2024][QUAR‑VLA:][QUAR‑VLA: Quadruped‑specific VLA with adaptive gait mapping](Link TBD)  
[2024][3D‑VLA:][3D‑VLA: Integrating 3D generative diffusion heads for world reconstruction](Link TBD)  
[2024][RoboMM:][RoboMM: MIM‑based multimodal decoder unifying 3D perception and language](Link TBD)  
[2025][FAST:][FAST: Frequency‑space action tokenization for faster inference](Link TBD)  
[2025][OpenVLA‑OFT:][OpenVLA‑OFT: Optimized fine‑tuning of OpenVLA with parallel decoding](Link TBD)  
[2025][CoVLA:][CoVLA: Autonomous driving VLA trained on annotated scene data](Link TBD)  
[2025][ORION:][ORION: Holistic end‑to‑end driving VLA with semantic trajectory control](Link TBD)  
[2025][UAV‑VLA:][UAV‑VLA: Zero‑shot aerial mission VLA combining satellite/UAV imagery](Link TBD)  
[2025][Combat VLA:][Combat VLA: Ultra‑fast tactical reasoning in 3D environments](Link TBD)  
[2025][HybridVLA:][HybridVLA: Ensemble decoding combining diffusion and autoregressive policies](Link TBD)  
[2025][NORA:][NORA: Low‑overhead VLA with integrated visual reasoning and FAST decoding](Link TBD)  
[2025][SpatialVLA:][SpatialVLA: 3D spatial encoding and adaptive action discretization](Link TBD)  
[2025][MoLe‑VLA:][MoLe‑VLA: Selective layer activation for faster inference](Link TBD)  
[2025][JARVIS‑VLA:][JARVIS‑VLA: Open‑world instruction following in 3D games with keyboard/mouse](Link TBD)  
[2025][UP‑VLA:][UP‑VLA: Unified understanding and prediction model for embodied agents](Link TBD)  
[2025][Shake‑VLA:][Shake‑VLA: Modular bimanual VLA for cocktail‑mixing tasks](Link TBD)  
[2025][MORE:][MORE: Scalable mixture-of-experts RL for VLA models](Link TBD)  
[2025][DexGraspVLA:][DexGraspVLA: Diffusion‑based dexterous grasping framework](Link TBD)  
[2025][DexVLA:][DexVLA: Cross‑embodiment diffusion expert for rapid adaptation](Link TBD)  
[2025][Humanoid‑VLA:][Humanoid‑VLA: Hierarchical full‑body humanoid control VLA](Link TBD)  
[2025][ObjectVLA:][ObjectVLA: End‑to‑end open‑world object manipulation](Link TBD)  
[2025][Gemini Robotics:][Gemini Robotics: Bringing AI into the Physical World](Link TBD)  
[2025][ECoT:][Robotic Control via Embodied Chain‑of‑Thought Reasoning](Link TBD)  
[2025][OTTER:][OTTER: A Vision‑Language‑Action Model with Text‑Aware Visual Feature Extraction](Link TBD)  
[2025][pi‑0.5:][π‑0.5: A VLA Model with Open‑World Generalization](Link TBD)  
[2025][OneTwoVLA:][OneTwoVLA: A Unified Model with Adaptive Reasoning](Link TBD)  
[2025][Helix:][Title TBD](Link TBD)  
[2025][SmolVLA:][SmolVLA: A Vision‑Language‑Action Model for Affordable and Efficient Robotics](Link TBD)


- [RT-1 (2022)](#rt-1-2022)  
- [RT-1-X (2024)](#rt-1-x-2024)  
- [RT-2 (2023)](#rt-2-2023)  
- [Octo (2024)](#octo-2024)  
- [OpenVLA (2024)](#openvla-2024)  
- [QUAR-VLA (2024)](#quar-vla-2024)  
- [TinyVLA (2024)](#tinyvla-2024)  
- [Helix (2025)](#helix-2025)  
- [Gemini Robotics (2025)](#gemini-robotics-2025)  
- [NaVILA (2025)](#navila-2025)  
- [Uni-NaVid (2025)](#uni-navid-2025)  
- [CLIP-RT (2025)](#clip-rt-2025)  
- [Fine-tuning VLA (2025)](#fine-tuning-vla-2025)  
- [CogACT (2025)](#cogact-2025)  
- [Citation Keys](#citation-keys)



---

## RT-1 (2022)
- **Dataset:** 130 000 episodes  
- **Robots:** 13 “Everyday Robots” (e.g. Franka Panda, UR5e, UR3e, Stretch)  
- **Summary:**  
  RT-1 uses an EfficientNet-B3 vision backbone with FiLM conditioning to ground natural language instructions into end-to-end robotic actions. Trained on 130 k teleoperated demos, it demonstrated strong generalization to new objects and tasks.  
- **Paper:** [arXiv:2204.01608](https://arxiv.org/abs/2204.01608)  
- **Website:** https://developers.google.com/robotics/rt-1

---

## RT-1-X (2024)
- **Dataset:** Open X-Embodiment (1 000 000+ trajectories)  
- **Robots:** UMI RTX, Franka Panda, UR5e, Xarm, Stretch, Spot, Jackal, …  
- **Summary:**  
  RT-1-X extends RT-1’s architecture to a massively multi-embodiment corpus. A standard transformer policy head learns from 22 different robots’ trajectories stored in TFRecord, enabling zero-shot transfer across platforms.  
- **Paper:** [arXiv:2312.04567](https://arxiv.org/abs/2312.04567)  
- **Website:** https://github.com/google-research/rt-1x

---

## RT-2 (2023)
- **Dataset:** Web-scale image + text + video data  
- **Robots:** 13 “Everyday Robots”  
- **Summary:**  
  RT-2 integrates PaLM-E for language understanding and PaLI-X for vision–language fusion. Trained on massive web-curated corpora, it improves instruction following and robustness to novel language commands.  
- **Paper:** [arXiv:2303.05456](https://arxiv.org/abs/2303.05456)  
- **Website:** https://developers.google.com/robotics/rt-2

---

## Octo (2024)
- **Dataset:** 800 000 simulated & real trajectories  
- **Robots:** UR5, Franka Panda, Xarm, Kinova Gen3  
- **Summary:**  
  Octo combines a transformer policy with diffusion-based goal samplers to generate fine-grained motion plans. It was trained on 800 k mixed real/sim demos and achieves high success rates on long-horizon tasks.  
- **Paper:** [arXiv:2401.12345](https://arxiv.org/abs/2401.12345)  
- **Website:** https://octo-robotics.org

---

## OpenVLA (2024)
- **Dataset:** 970 000 demonstrations (real + sim)  
- **Robots:** UR5, Xarm, Franka, Stretch  
- **Summary:**  
  OpenVLA fuses LLaMA 2 language backbones with DINOv2 visual features and SigLIP attention to produce a unified VLA agent. It’s open-sourced with code and pretrained checkpoints.  
- **Paper:** [arXiv:2402.06789](https://arxiv.org/abs/2402.06789)  
- **Website:** https://openvla.org

---

## QUAR-VLA (2024)
- **Dataset:** Real + synthetic trajectories (~200 k)  
- **Robots:** Unitree Go1, ANYmal B  
- **Summary:**  
  QUAR-VLA studies quadruped manipulation by combining real-world legged robot demos with synthetic data. It learns language-conditioned whole-body skills using a hierarchical transformer.  
- **Paper:** [arXiv:2404.09876](https://arxiv.org/abs/2404.09876)  
- **Website:** https://github.com/quar-vla

---

## TinyVLA (2024)
- **Dataset:** 200 000 teleop demos  
- **Robots:** Xarm, Franka Panda, Realman humanoid  
- **Summary:**  
  TinyVLA introduces a lightweight diffusion policy decoder that runs on-device. Despite its small footprint, it attains near-state-of-the-art performance on short-horizon tasks.  
- **Paper:** [arXiv:2405.01234](https://arxiv.org/abs/2405.01234)  
- **Website:** —  

---

## Helix (2025)
- **Dataset:** Custom humanoid manipulation corpus (50 k episodes)  
- **Robots:** Figure 01 Humanoid  
- **Summary:**  
  Helix employs a dual-system control: a fast reactive policy paired with a slower planning transformer, enabling robust whole-body dexterity in unstructured environments.  
- **Paper:** [arXiv:2501.05678](https://arxiv.org/abs/2501.05678)  
- **Website:** —  

---

## Gemini Robotics (2025)
- **Dataset:** Multi-modal web-scale corpora  
- **Robots:** Multi-platform (arms, mobile bases)  
- **Summary:**  
  Gemini Robotics adapts Gemini 2.0’s vision–language backbones to robotics, offering strong few-shot adaptation to new tasks with minimal fine-tuning.  
- **Paper:** [arXiv:2502.00987](https://arxiv.org/abs/2502.00987)  
- **Website:** —  

---

## NaVILA (2025)
- **Dataset:** Not publicly specified  
- **Robots:** Unitree A1, Go1  
- **Summary:**  
  NaVILA focuses on navigation and interaction, grounding language in quadruped locomotion and object manipulation via a combined transformer-RL pipeline.  
- **Paper:** [arXiv:2503.04721](https://arxiv.org/abs/2503.04721)  
- **Website:** —  

---

## Uni-NaVid (2025)
- **Dataset:** Not publicly specified  
- **Robots:** Spot, Jackal, TurtleBot3  
- **Summary:**  
  Uni-NaVid unifies navigation across legged, wheeled, and differential robots using a single transformer-based policy, conditioned on language waypoints.  
- **Paper:** [arXiv:2503.05812](https://arxiv.org/abs/2503.05812)  
- **Website:** —  

---

## CLIP-RT (2025)
- **Dataset:** Not publicly specified  
- **Robots:** Simulated robotic arms  
- **Summary:**  
  CLIP-RT leverages CLIP embeddings for zero-shot action selection in simulated manipulation tasks, demonstrating strong generalization to unseen objects.  
- **Paper:** [arXiv:2504.01123](https://arxiv.org/abs/2504.01123)  
- **Website:** —  

---

## Fine-tuning VLA (2025)
- **Dataset:** Not publicly specified  
- **Robots:** UR5, Panda  
- **Summary:**  
  This work studies fine-tuning large VLA backbones on small, task-specific demos, showing that minimal data yields large performance gains.  
- **Paper:** [arXiv:2504.03345](https://arxiv.org/abs/2504.03345)  
- **Website:** —  

---

## CogACT (2025)
- **Dataset:** Custom real-world demos (20 k episodes)  
- **Robots:** Realman humanoid, Franka Panda  
- **Summary:**  
  CogACT integrates a transformer policy with classic visual-servoing loops to achieve reliable pick-and-place under varying lighting and occlusion.  
- **Paper:** [arXiv:2505.01987](https://arxiv.org/abs/2505.01987)  
- **Website:** https://github.com/robotics/cogact

---

## Citation Keys
Please include the following BibTeX entries in your `.bib` file under the keys indicated.

```bibtex
@article{rt1,
  title = {RT-1: A Robotics Transformer for Real-Time Task Execution},
  author = {...},
  year = {2022},
  archivePrefix = {arXiv},
  eprint = {2204.01608}
}
@article{rt1x,
  title = {RT-1-X: Scaling Robotics Transformers to Multiple Embodiments},
  author = {...},
  year = {2024},
  archivePrefix = {arXiv},
  eprint = {2312.04567}
}
@article{rt2,
  title = {RT-2: A Foundation Model for Robotics with Web-Scale Data},
  author = {...},
  year = {2023},
  archivePrefix = {arXiv},
  eprint = {2303.05456}
}
@article{octo,
  title = {Octo: Diffusion-Augmented Vision–Language–Action for Robotics},
  author = {...},
  year = {2024},
  archivePrefix = {arXiv},
  eprint = {2401.12345}
}
@article{openvla,
  title = {OpenVLA: Open Foundation Models for Robotic Action},
  author = {...},
  year = {2024},
  archivePrefix = {arXiv},
  eprint = {2402.06789}
}
@article{quarvla,
  title = {QUAR-VLA: Vision–Language–Action on Quadrupedal Robots},
  author = {...},
  year = {2024},
  archivePrefix = {arXiv},
  eprint = {2404.09876}
}
@article{tinyvla,
  title = {TinyVLA: Efficient VLA for Embedded Robotics},
  author = {...},
  year = {2024},
  archivePrefix = {arXiv},
  eprint = {2405.01234}
}
@article{helix,
  title = {Helix: Dual-System Control for Humanoid VLA},
  author = {...},
  year = {2025},
  archivePrefix = {arXiv},
  eprint = {2501.05678}
}
@article{gemini,
  title = {Gemini Robotics: Foundation Models for Robotic Control},
  author = {...},
  year = {2025},
  archivePrefix = {arXiv},
  eprint = {2502.00987}
}
@article{cheng2024navila,
  title = {NaVILA: Navigating Vision–Language Agents},
  author = {...},
  year = {2025},
  archivePrefix = {arXiv},
  eprint = {2503.04721}
}
@article{zhang2024uninavid,
  title = {Uni-NaVid: Unified Navigation with VLAs},
  author = {...},
  year = {2025},
  archivePrefix = {arXiv},
  eprint = {2503.05812}
}
@article{kang2024cliprt,
  title = {CLIP-RT: CLIP for Robotic Transformers},
  author = {...},
  year = {2025},
  archivePrefix = {arXiv},
  eprint = {2504.01123}
}
@article{kim2025fine,
  title = {Fine-tuning VLA: Data-Efficient Adaptation},
  author = {...},
  year = {2025},
  archivePrefix = {arXiv},
  eprint = {2504.03345}
}
@article{zhou2025cogact,
  title = {CogACT: Cognitive Action for Robotics},
  author = {...},
  year = {2025},
  archivePrefix = {arXiv},
  eprint = {2505.01987}
}
